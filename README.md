# Neural Network Implementation from Scratch

This project implements a neural network from scratch using only NumPy and Pandas. The neural network is trained and tested on the MNIST dataset. The implementation includes the following steps:

1. Define the network architecture with input, hidden, and output layers.
2. Implement activation functions such as sigmoid, tanh, relu, and softmax.
3. Perform feedforward propagation to calculate the output of the network.
4. Implement backpropagation to update the weights and biases based on the error.
5. Train the neural network using the training data.
6. Test the trained network on the test data.
7. Calculate the confusion matrix and accuracy of the predictions.

## Dependencies

The implementation requires the following dependencies:

- NumPy
- Pandas
- scikit-learn (for data preprocessing and evaluation)
- Keras (for loading the MNIST dataset)

## Usage

1. Clone the repository or download the project files.
2. Open the `Tasneem_Muhammad_TeknoSoftInternship.ipynb` Jupyter Notebook.
3. Make sure the required dependencies are installed.
4. Run the notebook cells sequentially to execute the code.
5. The notebook trains the neural network on the MNIST dataset and evaluates its accuracy.
6. The confusion matrix for the predictions is also calculated.

## Results

The project demonstrates the implementation of a neural network from scratch and its application on the MNIST dataset. The accuracy of the model on the test data is printed as the output. Additionally, the confusion matrix provides insights into the performance of the neural network across different classes.

Feel free to explore the code and modify it for further experimentation or use in your own projects.
