{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project#2\n",
        "\n"
      ],
      "metadata": {
        "id": "pgA3KZeW5cXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using only NumPy and Pandas create a Neural Network from scratch(90 pt)\n",
        "<br> Set network architecture as follows:\n",
        "* Implement input, hidden, and output layers concerning input-output shape.\n",
        "* Define activation function.\n",
        "* Implement FeedForward\n",
        "* Implement BackPropagation\n",
        "* Implement Train and Test functions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FBcvy55Xz3_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test your model on both datasets then calculate the confusion matrix and accuracy\n",
        "* MNIST Dataset\n"
      ],
      "metadata": {
        "id": "JpL3HWZY0v1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "importing only pandas and numby"
      ],
      "metadata": {
        "id": "9PrBoV9qCZqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMoz8-T-zluV"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define error function"
      ],
      "metadata": {
        "id": "a1MDrxf_Co3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def loss(y_true, y_pred):\n",
        "    # Calculate the mean squared error loss between y_true and y_pred.\n",
        "    mse = np.mean(np.square(y_true - y_pred))\n",
        "    return mse\n",
        "\n",
        "def lossPrime(y_true, y_pred):\n",
        "    # Calculate the mean squared error loss between y_true and y_pred.\n",
        "    mse_prime =2*(y_pred - y_true) // np.size(y_true);\n",
        "    return mse_prime\n",
        "\n"
      ],
      "metadata": {
        "id": "NXrdpWuD0xLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "def softmax_derivative( x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "xNE1794FbRHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Network class"
      ],
      "metadata": {
        "id": "TK4UwTfaT-GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class neural network\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, sizeOfInput, sizeOfhidden, sizeOfOutput, activationFunction='sigmoid'):\n",
        "        self.sizeOfInput = sizeOfInput\n",
        "        self.sizeOfhidden = sizeOfhidden\n",
        "        self.sizeOfOutput = sizeOfOutput\n",
        "        self.activationFunction = activationFunction\n",
        "        #here I initialize my weights randomly\n",
        "        self.wOfInputs, self.bOfHidden, self.wOfOutput, self.bOfOutput = \\\n",
        "            self.InitializeWeightsRandomly()\n",
        "\n",
        "#Define some activation function and thier derivative\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self,x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def tanh(self,x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def tanh_derivative(self,x):\n",
        "        return 1 - np.tanh(x) ** 2\n",
        "\n",
        "    def relu(self,x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def relu_derivative(self,x):\n",
        "        return np.where(x > 0, 1, 0)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "        return exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "\n",
        "    #this is the function that initialize them randomly\n",
        "    def InitializeWeightsRandomly(self):\n",
        "        wOfInputs = np.random.randn(self.sizeOfInput, self.sizeOfhidden)\n",
        "        bOfHidden = np.zeros((1, self.sizeOfhidden))\n",
        "        wOfOutput = np.random.randn(self.sizeOfhidden, self.sizeOfOutput)\n",
        "        bOfOutput = np.zeros((1, self.sizeOfOutput))\n",
        "        return wOfInputs, bOfHidden, wOfOutput, bOfOutput\n",
        "\n",
        "    #the forward path\n",
        "    def forward(self, inputs):\n",
        "      HiddenInputs = np.dot(inputs, self.wOfInputs) + self.bOfHidden\n",
        "      if self.activationFunction == 'sigmoid':\n",
        "          HiddenOutput = sigmoid(HiddenInputs)\n",
        "      elif self.activationFunction == 'tanh':\n",
        "          HiddenOutput = tanh(HiddenInputs)\n",
        "      elif self.activationFunction == 'relu':\n",
        "          HiddenOutput = relu(HiddenInputs)\n",
        "      else:\n",
        "          raise ValueError(\"Invalid activation function\")\n",
        "\n",
        "      OutputOfInputs = np.dot(HiddenOutput, self.wOfOutput) + self.bOfOutput\n",
        "      FinalOutput = self.softmax(OutputOfInputs)\n",
        "\n",
        "      return HiddenOutput, FinalOutput\n",
        "\n",
        "\n",
        "    def backward(self, inputs, targets, HiddenOutput, FinalOutput): #backward\n",
        "\n",
        "        Error = targets - FinalOutput #first calculate error\n",
        "        DerivativeError = Error * self.softmax_derivative(FinalOutput) #then the derivative error with respect to oj\n",
        "\n",
        "        #then calculating the error in the hidden layer\n",
        "        if self.activationFunction == 'sigmoid':\n",
        "            ErrorInHidden = DerivativeError.dot(self.wOfOutput.T)\n",
        "            DerivativeErrorInHidden = ErrorInHidden * self.sigmoid_derivative(HiddenOutput)\n",
        "        elif self.activationFunction == 'tanh':\n",
        "            ErrorInHidden = DerivativeError.dot(self.wOfOutput.T)\n",
        "            DerivativeErrorInHidden = ErrorInHidden * self.tanh_derivative(HiddenOutput)\n",
        "        elif self.activationFunction == 'relu':\n",
        "            ErrorInHidden = DerivativeError.dot(self.wOfOutput.T)\n",
        "            DerivativeErrorInHidden = ErrorInHidden * self.relu_derivative(HiddenOutput)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid activation function\")\n",
        "\n",
        "        return DerivativeErrorInHidden, DerivativeError\n",
        "\n",
        "   #after calcultaing the error we need to get new w and b and update it\n",
        "    def UpdateNewWeights(self, inputs, DerivativeErrorInHidden, DerivativeError, HiddenOutput, LearningRate):\n",
        "        self.wOfOutput += HiddenOutput.T.dot(DerivativeError) * LearningRate\n",
        "        self.bOfOutput += np.sum(DerivativeError, axis=0, keepdims=True) * LearningRate\n",
        "        self.wOfInputs += inputs.T.dot(DerivativeErrorInHidden) * LearningRate\n",
        "        self.bOfHidden += np.sum(DerivativeErrorInHidden, axis=0, keepdims=True) * LearningRate\n",
        "\n",
        "\n",
        "    def train(self, inputs, targets, epochs=1000, LearningRate=0.01):\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(len(inputs)):\n",
        "                input_data = np.array(inputs[i]).reshape(1, -1)\n",
        "                target_data = np.array(targets[i]).reshape(1, -1)\n",
        "\n",
        "                HiddenOutput, FinalOutput = self.forward(input_data)\n",
        "\n",
        "                DerivativeErrorInHidden, DerivativeError = self.backward(input_data, target_data, HiddenOutput, FinalOutput)\n",
        "\n",
        "                self.UpdateNewWeights(input_data, DerivativeErrorInHidden, DerivativeError, HiddenOutput, LearningRate)\n",
        "\n",
        "    def test(self, inputs):\n",
        "        predictions = []\n",
        "        for i in range(len(inputs)):\n",
        "            input_data = np.array(inputs[i]).reshape(1, -1)\n",
        "            _, FinalOutput = self.forward(input_data)\n",
        "            predictions.append(FinalOutput[0])\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def evaluate(self, inputs, targets):\n",
        "        predictions = self.test(inputs)\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(len(predictions)):\n",
        "            predicted_class = np.argmax(predictions[i])\n",
        "            true_class = np.argmax(targets[i])\n",
        "\n",
        "            if predicted_class == true_class:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        accuracy = correct_predictions / len(targets)\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "YHJlcDfqUE_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test On MNIST"
      ],
      "metadata": {
        "id": "4y0BHhMuJJBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "\n",
        "mnist_inputs_train = train_images.reshape((60000, 28 * 28)).astype('float32') / 255.0\n",
        "mnist_inputs_test = test_images.reshape((10000, 28 * 28)).astype('float32') / 255.0\n",
        "\n",
        "\n",
        "mnist_targets_train = LabelBinarizer().fit_transform(train_labels)\n",
        "mnist_targets_test = LabelBinarizer().fit_transform(test_labels)\n",
        "\n",
        "\n",
        "mnist_inputs_train, mnist_inputs_test, mnist_targets_train, mnist_targets_test = train_test_split(\n",
        "    mnist_inputs_train, mnist_targets_train, test_size=0.2, random_state=42)\n",
        "\n",
        "mnist_nn = NeuralNetwork(sizeOfInput=784, sizeOfhidden=128, sizeOfOutput=10, activationFunction='sigmoid')\n",
        "mnist_nn.train(mnist_inputs_train, mnist_targets_train, epochs=50, LearningRate=0.1)\n",
        "\n",
        "\n",
        "mnist_accuracy = mnist_nn.evaluate(mnist_inputs_test, mnist_targets_test)\n",
        "print(\"MNIST Accuracy:\", mnist_accuracy * 100, \"%\")\n"
      ],
      "metadata": {
        "id": "oIXvEw-8LGxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix of MNIST"
      ],
      "metadata": {
        "id": "EhZmVhx-JOD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "mnist_predictions = mnist_nn.test(mnist_inputs_test)\n",
        "mnist_predicted_labels = np.argmax(mnist_predictions, axis=1)\n",
        "mnist_true_labels = np.argmax(mnist_targets_test, axis=1)\n",
        "\n",
        "\n",
        "mnist_conf_matrix = confusion_matrix(mnist_true_labels, mnist_predicted_labels)\n",
        "\n",
        "print(\"Confusion Matrix for MNIST:\")\n",
        "print(mnist_conf_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVb9jrqHJNlX",
        "outputId": "d44255c5-3535-4bd5-8e31-e2bbec9f82f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for MNIST:\n",
            "[[1067    0   11   12    4   22   27    6   20    6]\n",
            " [   0 1259   16    9    3    2    5    7   17    4]\n",
            " [  17   21  942   33   25   10   43   20   51   12]\n",
            " [  17    6   47  944    5   59   17   18   74   32]\n",
            " [   8    1   10    4  936   10   30   10   32  135]\n",
            " [  36    9   21   89   26  781   23   15   86   18]\n",
            " [  19    8   21    2   14   17 1063    2   27    4]\n",
            " [  11   22   28   21   14    9    1 1110   19   64]\n",
            " [  12   10   43   83   18   64   17   10  881   22]\n",
            " [   9    2   11   23   90   15    6   65   31  942]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Using built in libraries***\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0XKV3d-wpWAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras Neural Network with MNIIST"
      ],
      "metadata": {
        "id": "-5LmQTn_4-2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "NeuralNetworkKeras_MNIST = Sequential()\n",
        "NeuralNetworkKeras_MNIST.add(Dense(units=128, activation='relu', input_dim=784))\n",
        "NeuralNetworkKeras_MNIST.add(Dense(units=10, activation='softmax'))\n",
        "NeuralNetworkKeras_MNIST.compile(optimizer=Adam(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "mnist_targets_train_categorical = mnist_targets_train\n",
        "mnist_targets_test_categorical = mnist_targets_test\n",
        "\n",
        "NeuralNetworkKeras_MNIST.fit(mnist_inputs_train, mnist_targets_train_categorical, epochs=5, batch_size=32, verbose=0)\n",
        "mnist_accuracy_keras = NeuralNetworkKeras_MNIST.evaluate(mnist_inputs_test, mnist_targets_test_categorical, verbose=0)[1]\n",
        "\n",
        "\n",
        "print(\"MNIST Accuracy in my NeuralNetwork:\",mnist_accuracy * 100, \"%\")\n",
        "print(\"MNIST Accuracy in Keras Neural Network:\",mnist_accuracy_keras * 100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNnkXu0e4_Dr",
        "outputId": "21ab52b7-f48d-4821-f7db-bd7ef814460f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST Accuracy in my NeuralNetwork: 82.70833333333333 %\n",
            "MNIST Accuracy in Keras Neural Network: 94.9583351612091 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sklearn neural network with MNIST"
      ],
      "metadata": {
        "id": "alMANOFz4-QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "NeuralNetworkSklearn_MNIST = MLPClassifier(hidden_layer_sizes=(128,), max_iter=5, random_state=42)\n",
        "NeuralNetworkSklearn_MNIST.fit(mnist_inputs_train, mnist_targets_train)\n",
        "PredictMnist= NeuralNetworkSklearn_MNIST.predict(mnist_inputs_test)\n",
        "mnist_accuracy_sklearn = accuracy_score(mnist_targets_test, PredictMnist)\n",
        "mnist_targets_test_reshaped = np.array(to_categorical(mnist_targets_test)).reshape(len(mnist_targets_test), -1)\n",
        "print(\"MNIST Accuracy in my NeuralNetwork:\",mnist_accuracy * 100, \"%\")\n",
        "print(\"MNIST Accuracy in MLPClassifier:\", mnist_accuracy_sklearn * 100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZtGcAW_4-jv",
        "outputId": "cad556fa-376f-4dc5-a3d4-e51a75a654d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST Accuracy in my NeuralNetwork: 82.70833333333333 %\n",
            "MNIST Accuracy in MLPClassifier: 92.33333333333333 %\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}